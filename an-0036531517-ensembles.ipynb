{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Ensembles**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-26T12:58:24.214031Z","iopub.execute_input":"2024-05-26T12:58:24.214771Z","iopub.status.idle":"2024-05-26T12:58:24.730572Z","shell.execute_reply.started":"2024-05-26T12:58:24.214728Z","shell.execute_reply":"2024-05-26T12:58:24.729542Z"}}},{"cell_type":"markdown","source":"### **What is it?**\n**Ensemble methods** are techniques in machine learning that leverage the power of **multiple models** to enhance predictive performance. These methods combine various individual models to create a **more robust and accurate predictive model**. By aggregating the predictions of several models, ensemble methods aim to mitigate the weaknesses of individual models and **reduce the risk of overfitting**, ultimately leading to **improved accuracy** and robustness in predictions.","metadata":{}},{"cell_type":"markdown","source":"### **Why are they better**\n\n##### **Greater accuracy**\n* By combining the predictions of multiple models, ensemble methods can **reduce the errors** that are present in **individual models**. This combination allows for leveraging the **best aspects of each model**, resulting in more precise predictions.\n\n##### **Harder to overfit**\n* Individual models may be prone to overfitting, but using multiple models helps **reduce the likelihood** of any single **model overfitting** to the training data. This ensemble approach balances the tendencies of different models and creates a **more stable** and generalized **output**.\n\n##### **Better generalization**\n* Due to the aforementioned advantages, ensemble methods tend to **generalize better on new data**. This makes them **highly valuable** for practical applications where the model's performance on **unseen data is crucial**.","metadata":{}},{"cell_type":"markdown","source":"### **Examples**\n\n##### **Bagging**\n* Uses **sampling with replacement** to create different **subsets** of the data and **combines** their **predictions** (e.g., **Random Forest**).\n\n##### **Boosting**\n* Models are trained **iteratively**, and each model attempts to **correct the errors of previous** models. Examples include **AdaBoost**, **Gradient Boosting**, and **XGBoost** (we've already used 2 of these in previous notebooks).\n\n##### **Stacking**\n* Uses a **meta-model** that learns how to **combine the predictions of multiple different models**. These models are typically base models, and their **predictions** serve as the **input for the meta-model**. We will use **Meta-Learner**.\n\n##### **Voting**\n* Combines predictions by **\"voting\"**. It can use **\"hard voting\"** or **\"soft voting\"**. The former uses **majority voting** to make decisions, i.e., the majority decides, while **soft voting** selects the class with the **highest probability.**\n\nThere are also other principles like:\n* **Blending**\n* **Bucket of Models**\n* **Cascade Generalization**\nbut these will not be covered in this notebook.\n\n**Let's get to work!**","metadata":{}},{"cell_type":"markdown","source":"### Data loading","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport warnings\nimport logging\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:37:20.671546Z","iopub.execute_input":"2024-05-26T14:37:20.672015Z","iopub.status.idle":"2024-05-26T14:37:21.244302Z","shell.execute_reply.started":"2024-05-26T14:37:20.671978Z","shell.execute_reply":"2024-05-26T14:37:21.242711Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dapprojekt24-1/train.csv\n/kaggle/input/dapprojekt24-1/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Disable LightGBM info messages\nlogging.getLogger('lightgbm').setLevel(logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:37:21.722533Z","iopub.execute_input":"2024-05-26T14:37:21.723221Z","iopub.status.idle":"2024-05-26T14:37:21.730757Z","shell.execute_reply.started":"2024-05-26T14:37:21.723180Z","shell.execute_reply":"2024-05-26T14:37:21.728857Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/dapprojekt24-1/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/dapprojekt24-1/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:37:23.238287Z","iopub.execute_input":"2024-05-26T14:37:23.239328Z","iopub.status.idle":"2024-05-26T14:37:24.794179Z","shell.execute_reply.started":"2024-05-26T14:37:23.239260Z","shell.execute_reply":"2024-05-26T14:37:24.792922Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:37:24.796006Z","iopub.execute_input":"2024-05-26T14:37:24.796977Z","iopub.status.idle":"2024-05-26T14:37:24.820032Z","shell.execute_reply.started":"2024-05-26T14:37:24.796928Z","shell.execute_reply":"2024-05-26T14:37:24.818865Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"         Date Symbol  Adj Close      Close       High        Low       Open  \\\n0  2010-01-04    MMM  53.295380  83.019997  83.449997  82.669998  83.089996   \n1  2010-01-05    MMM  52.961575  82.500000  83.230003  81.699997  82.800003   \n2  2010-01-06    MMM  53.712681  83.669998  84.599998  83.510002  83.879997   \n3  2010-01-07    MMM  53.751179  83.730003  83.760002  82.120003  83.320000   \n4  2010-01-08    MMM  54.129955  84.320000  84.320000  83.300003  83.690002   \n\n      Volume  Target  Id  \n0  3043700.0       0   0  \n1  2847000.0       0   1  \n2  5268500.0       0   2  \n3  4470100.0       0   3  \n4  3405800.0       0   4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Symbol</th>\n      <th>Adj Close</th>\n      <th>Close</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Open</th>\n      <th>Volume</th>\n      <th>Target</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>MMM</td>\n      <td>53.295380</td>\n      <td>83.019997</td>\n      <td>83.449997</td>\n      <td>82.669998</td>\n      <td>83.089996</td>\n      <td>3043700.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-05</td>\n      <td>MMM</td>\n      <td>52.961575</td>\n      <td>82.500000</td>\n      <td>83.230003</td>\n      <td>81.699997</td>\n      <td>82.800003</td>\n      <td>2847000.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-06</td>\n      <td>MMM</td>\n      <td>53.712681</td>\n      <td>83.669998</td>\n      <td>84.599998</td>\n      <td>83.510002</td>\n      <td>83.879997</td>\n      <td>5268500.0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-07</td>\n      <td>MMM</td>\n      <td>53.751179</td>\n      <td>83.730003</td>\n      <td>83.760002</td>\n      <td>82.120003</td>\n      <td>83.320000</td>\n      <td>4470100.0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-08</td>\n      <td>MMM</td>\n      <td>54.129955</td>\n      <td>84.320000</td>\n      <td>84.320000</td>\n      <td>83.300003</td>\n      <td>83.690002</td>\n      <td>3405800.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:41:01.039398Z","iopub.execute_input":"2024-05-26T14:41:01.040153Z","iopub.status.idle":"2024-05-26T14:41:01.049541Z","shell.execute_reply.started":"2024-05-26T14:41:01.040097Z","shell.execute_reply":"2024-05-26T14:41:01.048018Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Models\nI will use two sets of models, one that I used in the mandatory notebook, and the second one that uses ensembles so we can make comparisons.","metadata":{}},{"cell_type":"code","source":"models = {\n    'GaussianNB': GaussianNB(),\n    'LogisticRegression': LogisticRegression(),\n    'XGBClassifier': XGBClassifier()\n}\n\nensembles = {\n    'Bagging_DT': BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42),\n    'Bagging_LR': BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, random_state=42),\n    'AdaBoost': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4), n_estimators=50, learning_rate=1.0, random_state=42),\n    'Voting': VotingClassifier(estimators=[\n        ('lr', LogisticRegression()),\n        ('dt', DecisionTreeClassifier()),\n        ('gnb', XGBClassifier())\n    ], voting='hard'),\n    'Stacking': StackingClassifier(estimators=[\n        ('lr', LogisticRegression()),\n        ('knn', KNeighborsClassifier())\n    ], final_estimator=LogisticRegression())\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:52:53.416753Z","iopub.execute_input":"2024-05-26T14:52:53.417196Z","iopub.status.idle":"2024-05-26T14:52:53.427280Z","shell.execute_reply.started":"2024-05-26T14:52:53.417153Z","shell.execute_reply":"2024-05-26T14:52:53.426210Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Data preparation","metadata":{}},{"cell_type":"code","source":"all_features = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume', 'Target']\nfeatures = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\ntrain_data1 = train_data[all_features]\ntrain_data1 = train_data1[(train_data1 >= 0).all(axis=1)]\n\ntrain_data1.fillna(-1, inplace=True)\n\nX = train_data1[features]\ny = train_data1['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:47:54.176522Z","iopub.execute_input":"2024-05-26T14:47:54.177152Z","iopub.status.idle":"2024-05-26T14:47:54.294038Z","shell.execute_reply.started":"2024-05-26T14:47:54.177086Z","shell.execute_reply":"2024-05-26T14:47:54.292964Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### First set of models","metadata":{}},{"cell_type":"code","source":"results1 = []\n\nbest_model1 = None\nbest_f1_score1 = 0\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rounded_predictions = (predictions >= 0.5).astype(int)\n    f1 = f1_score(y_test, rounded_predictions)\n    print(f'{name} - F1 score: {f1}')\n    \n    if f1 > best_f1_score1:\n        best_model1 = model\n        best_f1_score1 = f1\n\n    results1.append({'Model': name, 'F1 Score': f1})\n\nprint(f'Best model: {best_model1}')\nprint(f'Best F1 score: {best_f1_score1}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:48:00.697444Z","iopub.execute_input":"2024-05-26T14:48:00.697905Z","iopub.status.idle":"2024-05-26T14:48:04.526692Z","shell.execute_reply.started":"2024-05-26T14:48:00.697870Z","shell.execute_reply":"2024-05-26T14:48:04.525026Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"GaussianNB - F1 score: 0.877086296823139\nLogisticRegression - F1 score: 0.8784048361242393\nXGBClassifier - F1 score: 0.8798497080960251\nBest model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)\nBest F1 score: 0.8798497080960251\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### New set of models","metadata":{}},{"cell_type":"code","source":"results2 = []\n\nbest_model2 = None\nbest_f1_score2 = 0\n\nfor name, model in ensembles.items():\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rounded_predictions = (predictions >= 0.5).astype(int)\n    f1 = f1_score(y_test, rounded_predictions)\n    print(f'{name} - F1 score: {f1}')\n    \n    if f1 > best_f1_score2:\n        best_model2 = model\n        best_f1_score2 = f1\n\n    results2.append({'Model': name, 'F1 Score': f1})\n\nprint(f'Best model: {best_model2}')\nprint(f'Best F1 score: {best_f1_score2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:52:59.898986Z","iopub.execute_input":"2024-05-26T14:52:59.899483Z","iopub.status.idle":"2024-05-26T14:56:42.368561Z","shell.execute_reply.started":"2024-05-26T14:52:59.899446Z","shell.execute_reply":"2024-05-26T14:56:42.367139Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Bagging_DT - F1 score: 0.8622186619809298\nBagging_LR - F1 score: 0.8784048361242393\nAdaBoost - F1 score: 0.8781778209398069\nVoting - F1 score: 0.880048443687182\nStacking - F1 score: 0.8784048361242393\nBest model: VotingClassifier(estimators=[('lr', LogisticRegression()),\n                             ('dt', DecisionTreeClassifier()),\n                             ('gnb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=None, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            random_state=None, ...))])\nBest F1 score: 0.880048443687182\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Results**","metadata":{}},{"cell_type":"code","source":"if best_f1_score1 > best_f1_score2: best_model = best_model1\nelse: best_model = best_model2\nbest_f1_score = max(best_f1_score1, best_f1_score2)\n\nprint(f\"Best model in the notebook is: {best_model}\\n\")\nprint(f\"Best f1 score that I got is: {best_f1_score}\\n\")\n\nresults = pd.concat([pd.DataFrame(results1), pd.DataFrame(results2)], ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:56:51.876913Z","iopub.execute_input":"2024-05-26T14:56:51.877423Z","iopub.status.idle":"2024-05-26T14:56:51.912232Z","shell.execute_reply.started":"2024-05-26T14:56:51.877387Z","shell.execute_reply":"2024-05-26T14:56:51.910426Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Best model in the notebook is: VotingClassifier(estimators=[('lr', LogisticRegression()),\n                             ('dt', DecisionTreeClassifier()),\n                             ('gnb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=None, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            random_state=None, ...))])\n\nBest f1 score that I got is: 0.880048443687182\n\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                Model  F1 Score\n0          GaussianNB  0.877086\n1  LogisticRegression  0.878405\n2       XGBClassifier  0.879850\n3          Bagging_DT  0.862219\n4          Bagging_LR  0.878405\n5            AdaBoost  0.878178\n6              Voting  0.880048\n7            Stacking  0.878405","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GaussianNB</td>\n      <td>0.877086</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LogisticRegression</td>\n      <td>0.878405</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGBClassifier</td>\n      <td>0.879850</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bagging_DT</td>\n      <td>0.862219</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bagging_LR</td>\n      <td>0.878405</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>AdaBoost</td>\n      <td>0.878178</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Voting</td>\n      <td>0.880048</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Stacking</td>\n      <td>0.878405</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"I can happily tell you that one of my **ensemble models** has yielded the **best result**, and that's the **Voting model**. It achieved the most favorable outcome, which is quite fitting as I **utilized XGBoost**, the top performer among the base models, **along with others** to reach this **conclusion**.","metadata":{}},{"cell_type":"markdown","source":"##### Code for submission","metadata":{}},{"cell_type":"code","source":"X_test = test_data[features].fillna(-1)\nIDs = test_data['Id']\n\npredictions = best_model.predict(X_test)\nrounded_predictions = (predictions >= 0.5).astype(int)\n\nnegative_mask = (X_test < 0).any(axis=1)\nrounded_predictions[negative_mask] = 0\n\npredictions_df = pd.DataFrame({'Id': IDs, 'TARGET': rounded_predictions})\n\npredictions_df.to_csv('/kaggle/working/predictions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:57:35.686534Z","iopub.execute_input":"2024-05-26T14:57:35.687050Z","iopub.status.idle":"2024-05-26T14:57:38.481767Z","shell.execute_reply.started":"2024-05-26T14:57:35.687001Z","shell.execute_reply":"2024-05-26T14:57:38.480556Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}